
# coding: utf-8

# <img src="https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg" width=500,>
# <h3 style="text-align: center;"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>

# ---

# <h2 style="text-align: center;"><b>PyTorch. Основы: синтаксис, torch.cuda и torch.autograd</b></h2>

# <p style="align: center;"><img src="https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/pytorch-logo-dark.png", width=400, height=300></p>

# На этом занятии мы рассмотрим основы фреймворка глубокого обучения PyTorch.  
# 
# Когда хочется написать какую-нибудь нейросеть, решающую определённую задачу, будь то какая-нибудь простая классификация чего-либо или обнаружение лиц людей на видео. Всё, конечно, всегда начинается со **сбора данных**, а уже потом реализуются модели и проводятся эксперименты.  
# 
# Однако люди быстро поняли, что писать свои нейронные сети каждый раз с нуля ну очень уж долго и неразумно, поэтому придумали так называемые **фреймворки** - модули, в которых есть функционал, с помощью которого можно быстро и просто решать типовые задачи, и уже с помощью этих средств писать решения к более сложным задачам.

# Есть много различных фремворков глубокого обучения. Разница между ними прежде всего в том, каков общий принцип вычислений. 
# Например, в **Caffe и Caffe2** вы пишете код, по сути, составляя его из готовых "кусочков", как в Lego, в **TensorFlow и Theano** вы сначала объявляете вычислительный граф, потом компилируете его и запускаете (sees.run()), в то время как в **Torch и PyTorch** вы пишете почти точно так же, как на NumPy, а граф вычислений создаётся только при запуске (то есть существует только во время выполнения, потом он "разрушается"). **Keras** позволяет как строить блоки, так и компилировать свой граф:

# <p style="align: center;"><img src="https://habrastorage.org/web/e3e/c3e/b78/e3ec3eb78d714a7993a6b922911c0866.png", width=500, height=500></p>  
# <p style="text-align: center;"><i>Картинка взята из отличной [статьи на Хабре](https://habr.com/post/334380/)</i><p>

# <h3 style="text-align: center;"><b>Установка</b></h3>

# Инструкция по установке PyTorch есть на официальном [Github'e Deep Learning School](https://github.com/deepmipt/dlschl/wiki/%D0%98%D0%BD%D1%81%D1%82%D1%80%D1%83%D0%BA%D1%86%D0%B8%D1%8F-%D0%BF%D0%BE-%D1%83%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BA%D0%B5-PyTorch).

# <h3 style="text-align: center;">Синтаксис<b></b></h3>

# In[2]:


import torch


# Сначала немного фактов про PyTorch:  
# - динамический граф вычислений
# - удобные модули `torch.nn` и `torchvision для` написания нейросеток с минимальными усилиями
# - в некоторых задачах даже быстрее TensorFlow (но не во всех)  
# - легко проводить вычисления на GPU

# Если PyTorch представить формулой, то она будет такой:  
# 
# $$PyTorch = NumPy + CUDA + Autograd$$

# (CUDA - [wiki](https://ru.wikipedia.org/wiki/CUDA))

# Посмотрим, как в PyTorch выполняются операции с векторами.  
# 
# Напоминание: **тензором** называется многомерный вектор, то есть есть:  
# 
# x = np.array([1,2,3]) - вектор = тензор размерности 1 (то есть (1,))  
# y = np.array([[1, 2, 3], [4, 5, 6]]) - матрица = тензор размерности 2 (в данном случае тензор (2, 3))  
# z = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]],  
#     [[1, 2, 3], [4, 5, 6], [7, 8, 9]],  
#               [[1, 2, 3], [4, 5, 6], [7, 8, 9]]]) - "кубик" (3, 3, 3) = тензор размерности 3 (в данном случае (3, 3, 3))

# Простейшим примером 3-мерного тензора является **картинка** - это "параллелепипед" из чисел, у коготорого три размерности - высота, ширина и количество каналов, значит это тензор размерности 3.

# Понятие тензора нужно знать потому, что в PyTorch мы оперируем переменными типа `torch.Tensor` (`FloatTensor`, `IntTensor`, `ByteTensor`), и пугаться их названий совершенно не нужно - это просто векторы, у которых несколько размерностей.

# Все типы тензоров:

# In[3]:


torch.HalfTensor      # 16 бит, с плавающей точкой
torch.FloatTensor     # 32 бита,  с плавающей точкой
torch.DoubleTensor    # 64 бита, с плавающей точкой

torch.ShortTensor     # 16 бит, целочисленный, знаковый
torch.IntTensor       # 32 бита, целочисленный, знаковый
torch.LongTensor      # 64 бита, целочисленный, знаковый

torch.CharTensor      # 8 бит, целочисленный, знаковый
torch.ByteTensor      # 8 бит, целочисленный, беззнаковый


# Мы будем использовать только `torch.FloatTensor()` и `torch.IntTensor()`. 

# Перейдём к делу:

# * Создание тензоров:

# In[4]:


a = torch.FloatTensor([1, 2])
a


# In[5]:


a.shape


# In[6]:


b = torch.FloatTensor([[1,2,3], [4,5,6]])
b


# In[7]:


b.shape


# In[92]:


x = torch.FloatTensor(2,3,4)


# In[93]:


x


# In[10]:


x = torch.FloatTensor(100)
x


# In[11]:


x = torch.IntTensor(45, 57, 14, 2)
x.shape


# **Обратите внимание** - если вы создаёте тензор через задание размерностей (как в примере выше), то он изначально заполняюстя случайным "мусором". Что инициализировать нулями, нужно написать .zero_() в конце:

# In[12]:


x = torch.IntTensor(3, 2, 4)
x


# In[13]:


x = torch.IntTensor(3, 2, 4).zero_()
x


# Аналог функции `np.reshape()` == `torch.view()`:

# In[15]:


b.view(3, 2)


# In[16]:


b


# **Обратите внимание** - torch.view() создаёт новый тензор, а не изменяет старый!

# In[17]:


b.view(-1)


# In[18]:


b


# * Изменение типа тензора:

# In[19]:


a = torch.FloatTensor([1.5, 3.2, -7])


# In[20]:


a.type_as(torch.IntTensor())


# In[21]:


a.type_as(torch.ByteTensor())


# Обратите внимание, что при `.type_as()` создаётся новый тензор (старый не меняется), то есть это не in-place операция:

# In[22]:


a


# * Индексация точная такая же, как и в NumPy:

# In[23]:


a = torch.FloatTensor([[100, 20, 35], [15, 163, 534], [52, 90, 66]])
a


# In[24]:


a[0, 0]


# In[25]:


a[0][0]


# In[26]:


a[0:2, 0:2]


# ### Задача 1

# 1). Создайте два вещественных тензора: `a` размером (3, 4) и `b` размером (12,)   
# 2). Создайте тензор `c`, являющийся тензором `b`, но размера (2, 2, 3)  
# 3). Выведите первый столбец матрицы `a` с помощью индексации

# In[32]:


# Ваш код здесь
a = torch.FloatTensor(3,4)
b = torch.FloatTensor(12)
c = b.view(2,2,3)


# In[33]:


a


# In[34]:


b


# In[35]:


c


# In[38]:


a[:,0]


# **Арифметика и булевы операции** работаю также, как и в NumPy, **НО** лучше использовать не опреаторы `+`, `-`, `*`, `/`, а их аналоги:  
# 
# | Оператор | Аналог |
# |:-:|:-:|
# |`+`| `torch.add()` |
# |`-`| `torch.sub()` |
# |`*`| `torch.mul()` |
# |`/`| `torch.div()` |

# * Сложение:

# In[39]:


a = torch.FloatTensor([[1, 2, 3], [10, 20, 30], [100, 200, 300]])
b = torch.FloatTensor([[-1, -2, -3], [-10, -20, -30], [100, 200, 300]])


# In[40]:


a + b


# Лучше:

# In[41]:


a.add(b)


# In[42]:


b = -a
b


# In[43]:


a + b


# * Вычитание:

# In[44]:


a - b


# Лучше:

# In[45]:


a.sub(b)


# * Умножение (поэлементное):

# In[46]:


a * b


# Лучше:

# In[47]:


a.mul(b)


# * Деление (поэлементное):

# In[48]:


a = torch.FloatTensor([[1, 2, 3], [10, 20, 30], [100, 200, 300]])
b = torch.FloatTensor([[-1, -2, -3], [-10, -20, -30], [100, 200, 300]])


# In[49]:


a / b


# Лучше:

# In[50]:


a.div(b)


# Заметьте, все эти операции **не меняют исходные тензоры**, а **создают новые**:

# In[51]:


a


# In[52]:


b


# ### Задача 2

# 1). Создайте два вещественных тензора: `a` размером (5, 2) и `b` размером (1,10)   
# 2). Создайте тензор `c`, являющийся тензором `b`, но размера (5, 2)  
# 3). Произведите все арифметические операции с тензорами `a` и `c`

# In[57]:


# Ваш код здесь
a = torch.FloatTensor(range(5*2)).view(5,2)
b = torch.FloatTensor(range(1*10)).view(1,10)


# In[58]:


a


# In[59]:


b


# In[60]:


c = b.view(5,2)
c


# In[61]:


a.add(c)


# In[62]:


a.sub(c)


# In[63]:


a.mul(c)


# In[64]:


a.div(c)


# * **Операторы сравнения**:

# In[65]:


a = torch.FloatTensor([[1, 2, 3], [10, 20, 30], [100, 200, 300]])
b = torch.FloatTensor([[-1, -2, -3], [-10, -20, -30], [100, 200, 300]])


# In[66]:


a == b


# In[67]:


a != b


# In[68]:


a < b


# In[69]:


a > b


# * **Булевы маски**:

# In[70]:


a[a > b]


# In[71]:


b[a == b]


# Опять же, тензоры не меняются:

# In[72]:


a


# In[73]:


b


# Применение **стандартных функций** такое же, как и в numpy - поэлементное:

# In[74]:


a = torch.FloatTensor([[1, 2, 3], [10, 20, 30], [100, 200, 300]])


# In[75]:


a.sin()


# In[76]:


torch.sin(a)


# In[77]:


a.cos()


# In[78]:


a.exp()


# In[79]:


a.log()


# In[80]:


b = -a
b


# In[81]:


b.abs()


# **Сумма, среднее, максимум, минимум**:

# In[82]:


a.sum()


# In[83]:


a.mean()


# По осям:

# In[84]:


a.sum(0)


# In[85]:


a.sum(1)


# In[86]:


a.max()


# In[87]:


a.max(0)


# In[88]:


a.min()


# In[89]:


a.min(0)


# Обратите внимание - второй тензор при вызове функций .max() и .min() - это индексы этих максимальных/минимальных элементов по указанной размерности (то есть в данном случае a.min() вернул (1, 2, 3) - минимумы по 0 оси (по столбцам), и их индексы по 0-ой оси (0,0,0) (номер каждого элемента в своём столбце)).

# ### Задача 3

# Создайте тензор `a` размерности (100, 780, 780, 3) (можно интерпретировать это как 100 картинок размера 780х780 с тремя цветовыми каналами) и выведите первый элемент этого тензора как картинку (с помощью matplotlib.pyplot).
# 
# Выведите среднее элементов по 1-ой оси (по сути - средняя картинка по всем картинкам) и по 4-ой оси (по сути - усреднение каналов для каждой картинки).

# In[105]:


# Ваш код здесь
a = torch.FloatTensor(range(100*780*780*3)).view(100, 780, 780, 3)


# In[106]:


a[0]


# In[114]:


import numpy as np
import pandas as pd
import matplotlib as pyplot

from scipy import ndimage #спецификатор для работы с изображениями
from scipy import misc 
from PIL import Image

get_ipython().run_line_magic('matplotlib', 'inline')

pyplot.pyplot.imshow(a[99])


# In[115]:


pyplot.pyplot.imshow(a.mean(0))


# In[117]:


a.mean(3)


# **Матричные операции:**

# In[ ]:


z = x.mm(y)
z = torch.mm(x, y)
Матричное умножение.
z = x.mv(v)
z = torch.mv(x, v)
Умножение матрицы на вектор.
z = x.dot(y)
z = torch.dot(x, y)
Скалярное умножение тензоров.
bz = bx.bmm(by)
bz = torch.bmm(bx, by)
Перемножает матрицы целыми батчами.


# * Транспонирование матрицы (тензора):

# In[120]:


a = torch.FloatTensor([[1, 2, 3], [10, 20, 30], [100, 200, 300]])
a


# In[121]:


a.t()


# И снова - сам тензор не меняется (то есть при вызове создаётся новый):

# In[122]:


a


# * Скалярное произведение векторов (1-мерных тензоров):

# In[123]:


a = torch.FloatTensor([1, 2, 3, 4, 5, 6])
b = torch.FloatTensor([-1, -2, -4, -6, -8, -10])


# In[124]:


a.dot(b)


# In[125]:


a @ b


# In[126]:


type(a)


# In[127]:


type(b)


# In[128]:


type(a @ b)


# * Матричное умножение:

# In[129]:


a = torch.FloatTensor([[1, 2, 3], [10, 20, 30], [100, 200, 300]])
b = torch.FloatTensor([[-1, -2, -3], [-10, -20, -30], [100, 200, 300]])


# In[130]:


a.mm(b)


# In[131]:


a @ b


# Тензоры неизменны:

# In[132]:


a


# In[133]:


b


# In[134]:


a = torch.FloatTensor([[1, 2, 3], [10, 20, 30], [100, 200, 300]])
b = torch.FloatTensor([[-1], [-10], [100]])


# In[135]:


print(a.shape, b.shape)


# In[136]:


a @ b


# Если "развернуть" тензор b просто в массив элементов (`torch.view(-1)`), умножение будет как на столбец:

# In[137]:


b


# In[138]:


b.view(-1)


# In[139]:


a @ b.view(-1)


# In[140]:


a.mv(b.view(-1))


# **Перевод из NumPy в PyTorch**:

# In[141]:


import numpy as np

a = np.random.rand(3, 3)
a


# In[142]:


b = torch.from_numpy(a)
b


# **НО!** Обратите внимание - a и b в этом случае будут использовать одно и то же хранилище данных, то есть измение одного тензора будет менять и другой:

# In[143]:


b -= b
b


# In[144]:


a


# **Перевод из PyTorch в NumPy:**

# In[145]:


a = torch.FloatTensor(2, 3, 4)
a


# In[146]:


type(a)


# In[147]:


x = a.numpy()
x


# In[148]:


x.shape


# In[149]:


type(x)


# ### Задача 4

# Напишите функцию `forward_pass(X, w)` ($w_0$ входит в $w$) для одного нейрона (с сигмоидой) с помощью PyTorch (у вас уже должен быть код на NumPy).

# In[174]:


def sigmoid(x):
    """Сигмоидальная функция"""
    return 1 / (1 + np.exp(-x))


# In[177]:


def forward_pass(X, w):
    # Ваш код здесь
    n = X.shape[0]
    y_pred = torch.FloatTensor(n, 1)
    y_pred = sigmoid(X.mm(w).view(-1,1))
    return(y_pred)


# In[187]:


X = torch.FloatTensor([[-5, 5], [2, 3], [1, -1]])
w = torch.FloatTensor([[-0.5], [2.5]])
result = forward_pass(X, w)
print('result: {}'.format(result))


# Должно получиться: 
# 
# |variable|value|
# |-|-|
# |**X**|torch.FloatTensor([[-5, 5], [15, 20], [100, -700]])|
# |**w**|torch.FloatTensor([[-0.5], [150]])|
# |**result**|torch.FloatTensor([[1.0000], [0.9985], [0.0474]])|   

# <h3 style="text-align: center;">[CUDA](https://ru.wikipedia.org/wiki/CUDA)<b></b></h3>

# [Краткое видео про то, как GPU используется в обучении нейросетей](https://www.youtube.com/watch?v=EobhK0UZm80)

# Все вычисления в PyTorch можно проводить как на CPU, так и на GPU (Graphical Processing Unit) (если она у вас есть). В PyTorch переключение между ними делается очень просто, что является одной из ключевых его особенностей.

# In[188]:


x = torch.FloatTensor(1024, 1024).uniform_()
x


# In[189]:


x.is_cuda


# Переместим на GPU:

# In[190]:


x = x.cuda()


# In[191]:


x.is_cuda


# In[ ]:


x


# Перемножим две тензора на GPu и вернём результат вычисления на CPU:

# In[ ]:


a = torch.FloatTensor(10000, 10000).uniform_()
b = torch.FloatTensor(10000, 10000).uniform_()
c = a.cuda().mul(b.cuda()).cpu()


# In[ ]:


c


# In[ ]:


a


# Тензоры, лежащие на CPU, и тензоры, лежащие на GPU, недоступны друг для друга:

# In[ ]:


a = torch.FloatTensor(1000, 1000).uniform_().cpu()
b = torch.FloatTensor(10000, 10000).uniform_().cuda()


# In[ ]:


a + b


# Вот ещё немного про то, как можно работать с GPU:

# In[ ]:


x = torch.FloatTensor(5, 5, 5).uniform_()

# проверяем, есть ли CUDA (то есть NVidia GPU)
if torch.cuda.is_available():
    # так можно получить имя устройства, которое связано с CUDA
    # (полезно в случае с несколькими видеокартами)
    device = torch.device('cuda')          # CUDA-device объект
    y = torch.ones_like(x, device=device)  # создаём тензор на GPU
    x = x.to(device)                       # тут можно просто ``.to("cuda")``
    z = x + y
    print(z)
    # с помощью``.to`` можно и изменить тип при перемещении
    print(z.to("cpu", torch.double))


# <h3 style="text-align: center;">Autograd<b></b></h3>

# Расшифровывается как Automatic Gradients (автоматическое взятие градиентов) - собственно, из названия понятно, что это модуль PyTorch, отвечающий за взятие производных.  
# 
# Возможно, для вас это бдет шок, но PyTorch (и любой фреймворк глубокого обучения) может продифференцировать функцию практически любой сложности.

# Импортируем нужный класс:

# In[192]:


from torch.autograd import Variable


# Идея такая: оборачиваем тензор в класс Variable(), получаем тоже тензор, но он имеет способность вычислять себе градиенты.  

# Если а - тензор, обёрнутый в Variable(), то при вызове a.backward() берутся градиенты по всем переменным, от которых зависит тензор a.

# **ВНИМАНИЕ!**  
# 
# Если вы используете версию `pytorch 0.4.0` или более новую, то ***`torch.Tensor` и `torch.Variable` - одно и то же!*** То есть Вам больше не нужно оборачивать в `Variable()`, чтобы брать градиенты - они берутся и по `Tensor()` (`torch.Variable()` - deprecated).

# Примеры:

# In[205]:


x = torch.FloatTensor(3, 1).uniform_()
w = torch.FloatTensor(3, 3).uniform_() 
b = torch.FloatTensor(3, 1).uniform_()

x = Variable(x, requires_grad=True)
w = Variable(w, requires_grad=True)
b = Variable(b, requires_grad=False)

y = (w @ x).add_(b)

loss = y.sum()

# берём градиенты по всем "листьям" - в данном случае это тензоры x, w и b
loss.backward()


# In[206]:


x.grad


# In[207]:


w.grad


# In[208]:


b.grad


# In[209]:


y.grad


# **Обратите внимание** - градиенты лежат в поле `.grad` у тех тензоров (Variable'ов), по которым брали эти градиенты. Градиенты **не лежат** в той Variable, от котороый они брались!

# Получить тензор из `Variable()` можно с помощью поля `.data`:

# In[210]:


x


# In[211]:


x.data


# ### Задача 5

# - Объявите тензор `a` размера (2, 3, 4) и тензор `b` размера (1, 8, 3), иницилизируйте их случайно равномерно (`.uniform_()`, как в примере выше)
# - Создайте их копии на GPU, выведите их сумму и разность
# - Затем измените форму тензора `b`, чтобы она совпадала с формой тензора `a`, получите тензор `c`  
# - Переместите `c` на CPU, переместите `a` на CPU  
# - Оберните их в `Variable()`
# - Объявите тензор `L = torch.mean((c - a) `**` 2)` и посчитайте градиент `L` по `c` ( то есть $\frac{\partial{L}}{\partial{c}})$
# - Получите тензор из `c` (то есть сейчас `c` - объект типа `Variable()`, вам нужно получить из него `FloatTensor()`)

# In[ ]:


# Ваш код здесь


# <h3 style="text-align: center;">Полезные ссылки:<b></b></h3>

# *1). Мегаполезная статья по PyTorch (на русском), на её основе делался этот ноутбук: https://habr.com/post/334380/*

# *2). Туториалы от самих разработчиков фреймворка: https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py*

# *3). Статья на arXiv о сравнении фреймворков глубокого обучения: https://arxiv.org/pdf/1511.06435.pdf*

# 4). *Ещё туториалы: https://github.com/yunjey/pytorch-tutorial*

# *5). Сайт Facebook AI Research - отдела, который разрабатывает PyTorch и другие крутые вещи в AI: https://facebook.ai/developers/tools*
