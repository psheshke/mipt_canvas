{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" width=500, height=450>\n",
    "<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W12dfswC9i-H"
   },
   "source": [
    "<h2 style=\"text-align: center;\"><b>Нейрон с сигмоидой</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rjXJZnV49i-K"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ATVkJLXp9i_c"
   },
   "source": [
    "Напомним, что сигмоидальная функция (сигмоида) выглядит так:  \n",
    "    \n",
    "$$\\sigma(x)=\\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*IDAnCFoeXqWL7F4u9MJMtA.png\" width=500px height=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bnQb9i619i_c"
   },
   "source": [
    "В данном случае мы снова решаем задачу бинарной классификации (2 класса: 1 или 0), но здесь уже будет другая функция активации:\n",
    "\n",
    "$$MSE\\_Loss(\\hat{y}, y) = \\frac{1}{n}\\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2 = \\frac{1}{n}\\sum_{i=1}^{n} (\\sigma(w \\cdot X_i) - y_i)^2$$ \n",
    "\n",
    "Здесь $w \\cdot X_i$ - скалярное произведение, а $\\sigma(w \\cdot X_i) =\\frac{1}{1+e^{-w \\cdot X_i}} $ - сигмоида.  \n",
    "\n",
    "**Примечание:** В формуле предполагается, что $b$ - свободный член - является частью вектора весов: $w_0$. Тогда, если к $X$ приписать слева единичный столбец, в скалярном произведении $b$ будет именно как свободный член. При реализации класса $b$ нужно считать отдельно (чтобы было нагляднее)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w46AyfrC9i_d"
   },
   "source": [
    "Можем взять производную лосса по весам и спускаться в пространстве весов в направлении наискорейшего убывания функции лосса. Формула для обновления весов в градиентном спуске такая:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w^{j+1} = w^{j} - \\alpha \\frac{\\partial Loss}{\\partial w} (w^{j})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w^j$ -- вектор весов на $j$-ой итерации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распишем дальше:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Для веса $w_j$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial Loss}{\\partial w_j} = \n",
    "\\frac{2}{n} \\sum_{i=1}^n \\left(\\sigma(w \\cdot x_i) - y_i\\right)(\\sigma(w \\cdot x_i))_{w_j}' = \\frac{2}{n} \\sum_{i=1}^n \\left(\\sigma(w \\cdot x_i) - y_i\\right)\\sigma(w \\cdot x_i)(1 - \\sigma(w \\cdot x_i))x_{ij}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Градиент $Loss$'а по вектору весов -- это вектор, $j$-ая компонента которого равна $\\frac{\\partial Loss}{\\partial w_j}$ (помним, что весов всего $m$):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "    \\frac{\\partial Loss}{\\partial w} &= \\begin{bmatrix}\n",
    "           \\frac{2}{n} \\sum_{i=1}^n \\left(\\sigma(w \\cdot x_i) - y_i\\right)\\sigma(w \\cdot x_i)(1 - \\sigma(w \\cdot x_i))x_{i1} \\\\\n",
    "           \\frac{2}{n} \\sum_{i=1}^n \\left(\\sigma(w \\cdot x_i) - y_i\\right)\\sigma(w \\cdot x_i)(1 - \\sigma(w \\cdot x_i))x_{i2} \\\\\n",
    "           \\vdots \\\\\n",
    "           \\frac{2}{n} \\sum_{i=1}^n \\left(\\sigma(w \\cdot x_i) - y_i\\right)\\sigma(w \\cdot x_i)(1 - \\sigma(w \\cdot x_i))x_{im}\n",
    "         \\end{bmatrix}\n",
    "\\end{align}=\\frac{1}{n} X^T (\\sigma(w \\cdot X) - y)\\sigma(w \\cdot X)(1 - \\sigma(w \\cdot X))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YjagNMyO9i_d"
   },
   "source": [
    "Реализуем сигмоиду и её производную:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EKkpHD_k9i_e"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Сигмоидальная функция\"\"\"\n",
    "    <Ваш код здесь>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S3V5xryB9i_g"
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Производная сигмоиды\"\"\"\n",
    "    <Ваш код здесь>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMJYeujp9i_h"
   },
   "source": [
    "Теперь нужно написать нейрон с сигмоидной функцией активации. Здесь всё очень похоже на перцептрон, но будут по-другому обновляться веса и другая функция активации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_pred, y):\n",
    "    '''\n",
    "    Считаем среднеквадратичную ошибку\n",
    "    '''\n",
    "    <Ваш код здесь>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "06LN5PS79i_i"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, w=None, b=0):\n",
    "        \"\"\"\n",
    "        :param: w -- вектор весов\n",
    "        :param: b -- смещение\n",
    "        \"\"\"\n",
    "        # пока что мы не знаем размер матрицы X, а значит не знаем, сколько будет весов\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "  \n",
    "    def activate(self, x):\n",
    "        <Ваш код здесь>\n",
    "        \n",
    "    def forward_pass(self, X):\n",
    "        \"\"\"\n",
    "        Рассчитывает ответ нейрон при предъявлении набора объектов\n",
    "        :param: X -- матрица примеров размера (n, m), каждая строка - отдельный объект\n",
    "        :return: вектор размера (n, 1) из нулей и единиц с ответами нейрона\n",
    "        \"\"\"\n",
    "        <Ваш код здесь>\n",
    "    \n",
    "    def backward_pass(self, X, y, y_pred, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Обновляет значения весов нейрон в соответствии с этим объектом\n",
    "        :param: X -- матрица входов размера (n, m)\n",
    "                y -- вектор правильных ответов размера (n, 1)\n",
    "                learning_rate - \"скорость обучения\" (символ alpha в формулах выше)\n",
    "        В этом методе ничего возвращать не нужно, только правильно поменять веса\n",
    "        с помощью градиентного спуска.\n",
    "        \"\"\"\n",
    "        <Ваш код здесь>\n",
    "    \n",
    "    def fit(self, X, y, num_epochs=5000):\n",
    "        \"\"\"\n",
    "        Спускаемся в минимум\n",
    "        :param: X -- матрица объектов размера (n, m)\n",
    "                y -- вектор правильных ответов размера (n, 1)\n",
    "                num_epochs -- количество итераций обучения\n",
    "        :return: loss_values -- вектор значений функции потерь\n",
    "        \"\"\"\n",
    "        self.w = np.zeros((X.shape[1], 1))  # столбец (m, 1)\n",
    "        self.b = 0  # смещение\n",
    "        loss_values = []  # значения функции потерь на различных итерациях обновления весов\n",
    "        \n",
    "        for i in range(num_epochs):\n",
    "            <Ваш код здесь>\n",
    "        \n",
    "        return loss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kYevHjQE9i_k"
   },
   "source": [
    "**Тестирование нейрона**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oiXg900-9i_m"
   },
   "source": [
    "Протестировуем новый нейрон **на тех же данных** по аналогии с тем, как это было проделано с перцептроном.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NgXCHG_P9i_n"
   },
   "source": [
    "**Проверка forward_pass()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BcrKIaLb9i_n"
   },
   "outputs": [],
   "source": [
    "w = np.array([1., 2.]).reshape(2, 1)\n",
    "b = 2.\n",
    "X = np.array([[1., 3.],\n",
    "              [2., 4.],\n",
    "              [-1., -3.2]])\n",
    "\n",
    "neuron = Neuron(w, b)\n",
    "y_pred = neuron.forward_pass(X)\n",
    "print (\"y_pred = \" + str(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pw9aByzb9i_t"
   },
   "source": [
    "**Проверка backward_pass()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJKVJCsB9i_t"
   },
   "outputs": [],
   "source": [
    "y = np.array([1, 0, 1]).reshape(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WCc5c44a9i_u"
   },
   "outputs": [],
   "source": [
    "neuron.backward_pass(X, y, y_pred)\n",
    "\n",
    "print (\"w = \" + str(neuron.w))\n",
    "print (\"b = \" + str(neuron.b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWIHm_PP39mx"
   },
   "source": [
    "Посмотрим, как меняется функция потерь в течение процесса обучения на реальных данных - датасет \"Яблоки и Груши\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7PIOizom9i_w"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/apples_pears.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MHzVksit9i_y"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1RsB6C1w9i_0"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=data['target'], cmap='rainbow')\n",
    "plt.title('Яблоки и груши', fontsize=15)\n",
    "plt.xlabel('симметричность', fontsize=14)\n",
    "plt.ylabel('желтизна', fontsize=14)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oLdRWfRD9i_3"
   },
   "source": [
    "Обозначим, что здесь признаки, а что - классы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1yrSFfII9i_3"
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:,:2].values  # матрица объекты-признаки\n",
    "y = data['target'].values.reshape((-1, 1))  # классы (столбец из нулей и единиц)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8W6RmOJT9i_5"
   },
   "source": [
    "**Вывод функции потерь**  \n",
    "Функция потерь должна убывать и в итоге стать близкой к 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6N-LhHXb9i_5"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "neuron = Neuron()\n",
    "Loss_values = neuron.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(Loss_values)\n",
    "plt.title('Функция потерь', fontsize=15)\n",
    "plt.xlabel('номер итерации', fontsize=14)\n",
    "plt.ylabel('$Loss(\\hat{y}, y)$', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d6vluBuL9i_-"
   },
   "source": [
    "Посмотрим, как нейрон классифицировал объекты из выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DI1TqgsE9i_-"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=np.array(neuron.forward_pass(X) > 0.5).ravel(), cmap='spring')\n",
    "plt.title('Яблоки и груши', fontsize=15)\n",
    "plt.xlabel('симметричность', fontsize=14)\n",
    "plt.ylabel('желтизна', fontsize=14)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B1HWkCbQ39nL"
   },
   "source": [
    "На самом деле то, что вы здесь наблюдаете (плохое качество разделения) -- последствие **затухающих градиентов (vanishing gradients)**. Мы позже ещё поговорим об этом (то есть о том, почему это происходит в данном случае).\n",
    "\n",
    "Попробуем увеличить количество итераций градиентного спуска (50k итераций):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfmaMG1X39nN"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "neuron = <Ваш код здесь>\n",
    "loss_values = <Ваш код здесь>  # num_epochs=50000\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=np.array(neuron.forward_pass(X) > 0.5).ravel(), cmap='spring')\n",
    "plt.title('Яблоки и груши', fontsize=15)\n",
    "plt.xlabel('симметричность', fontsize=14)\n",
    "plt.ylabel('желтизна', fontsize=14)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gJBRZxea39nT"
   },
   "source": [
    "Что ж, стало лучше. Градиенты \"текут\" медленно, но верно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2KkEFqF339n6"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Полезные ссылки</b></h3>\n",
    "\n",
    "0). Статья от Стэнфорда: http://cs231n.github.io/neural-networks-1/\n",
    "\n",
    "1). Хорошая статья про функции активации: https://www.jeremyjordan.me/neural-networks-activation-functions/\n",
    "\n",
    "2). [Видео от Siraj Raval](https://www.youtube.com/watch?v=-7scQpJT7uo)\n",
    "\n",
    "3). Современная статья про функции активации. Теперь на хайпе активация **$swish(x) = x\\sigma (\\beta x)$:** https://arxiv.org/pdf/1710.05941.pdf (кстати, при её поиске в некоторой степени использовался neural architecture search)\n",
    "\n",
    "4). **SeLU** имеет очень интересные, доказанные с помощью теории вероятностей свойства: https://arxiv.org/pdf/1706.02515.pdf (да, в этой статье 102 страницы)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "zoU3iRmV9jAB",
    "UcZXkrFy9jAC",
    "MQtbKUB69jAF"
   ],
   "name": "[sem_solution]perceptron_neuron.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
