{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUWCAY5opP87"
   },
   "source": [
    "<p style=\"align: center;\"><img align=center src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\"  width=500 height=400></p>\n",
    "\n",
    "<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sEkVD5qHpP89"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wj5MrpmRpP89"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Элементы теории оптимизации. Производные и частные производные.</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O1xIGzO5pP8-"
   },
   "source": [
    "<p style=\"text-align: center;\">(На основе https://github.com/romasoletskyi/Machine-Learning-Course)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODaZDX75pP8-"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Приращение линейной функции</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tldAx431pP8_"
   },
   "source": [
    "Давайте рассмотрим линейную функцию $y=kx+b$ и построим график: <br>  \n",
    "\n",
    "![source: Wikipedia](https://upload.wikimedia.org/wikipedia/commons/c/c1/Wiki_slope_in_2d.svg) <br>  \n",
    "\n",
    "Введём понятие **приращения** функции в точке $(x, y)$ как отношение вертикального изменения (измненеия функции по вертикали) $\\Delta y$ к горизонтальному изменению $\\Delta x$ и вычислим приращение для линейной функции:  \n",
    "\n",
    "$$приращение (\"slope\")=\\frac{\\Delta y}{\\Delta x}=\\frac{y_2-y_1}{x_2-x_1}=\\frac{kx_2+b-kx_1-b}{x_2-x_1}=k\\frac{x_2-x_1}{x_2-x_1}=k$$  \n",
    "\n",
    "Видим, что приращение в точке у прямой не зависит от $x$ и $\\Delta x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObcFoC9JpP9A"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Приращение произвольной функции</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dNfWm09WpP9A"
   },
   "source": [
    "Но что, если функция не линейная, а произвольная $f(x)$?  \n",
    "В таком случае просто нарисуем **касательную ** в точке, в которой ищем приращение, и будем смотреть уже на приращение касательной. Так как касательная - это прямая, мы уже знаем, какое у неё приращение (см. выше).\n",
    "![source: Wikipedia](https://upload.wikimedia.org/wikipedia/commons/d/d2/Tangent-calculus.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "137KHuBjpP9B"
   },
   "source": [
    "Имея граик функции мы, конечно, можем нарисовать касательную в точке. Но часто функции заданы аналитически, и хочется уметь сразу быстро получать формулу для приращения функциии в точке. Тут на помощь приходит **производная**.  Давайте посмотрим на определение производной его с нашим понятием приращения:  \n",
    "\n",
    "$$f'(x) = \\lim_{\\Delta x \\to 0}\\frac{\\Delta y}{\\Delta x} = \\lim_{\\Delta x \\to 0}\\frac{f(x + \\Delta x) - f(x)}{\\Delta x}$$  \n",
    "\n",
    "То есть по сути, значение производной функции в точке - это и есть приращение функции, если мы стремим длину отрезка $\\Delta x$ к нулю."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7YksIkmlpP9C"
   },
   "source": [
    "Посомтрим на интерактивное демо, демонстрирующее стремление $\\Delta x$ к нулю (*в Google Colab работать не будет!*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 764,
     "status": "error",
     "timestamp": 1540841307892,
     "user": {
      "displayName": "Илья Дмитриевич Захаркин",
      "photoUrl": "",
      "userId": "09157257912804633784"
     },
     "user_tz": -180
    },
    "id": "v9rhGojJpP9D",
    "outputId": "ba6596ca-53ac-45aa-ea53-affe48ac21ac"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R4DbxljwpP9F"
   },
   "outputs": [],
   "source": [
    "# pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJ_xbrHXpP9I"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a88b148aaf483fbfa1e5b2217c266c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='lg_z', max=4.0, min=-0.5), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(lg_z=(-0.5,4.0,0.1))\n",
    "def f(lg_z=1.0):\n",
    "    z = 10 ** lg_z\n",
    "    x_min = 1.5 - 6/z\n",
    "    x_max = 1.5 + 6/z\n",
    "    l_min = 1.5 - 4/z\n",
    "    l_max = 1.5 + 4/z\n",
    "    xstep = (x_max - x_min)/100\n",
    "    lstep = (l_max - l_min)/100\n",
    "    \n",
    "    x = np.arange(x_min, x_max, xstep)\n",
    "    \n",
    "    plt.plot(x, np.sin(x), '-b')     \n",
    "    \n",
    "    plt.plot((l_min,l_max), (np.sin(l_min), np.sin(l_max)), '-r')\n",
    "    plt.plot((l_min,l_max), (np.sin(l_min), np.sin(l_min)), '-r')\n",
    "    plt.plot((l_max,l_max), (np.sin(l_min), np.sin(l_max)), '-r')\n",
    "    \n",
    "    yax = plt.ylim()    \n",
    "    \n",
    "    plt.text(l_max + 0.1/z, (np.sin(l_min) + np.sin(l_max)) / 2, \"$\\Delta y$\")\n",
    "    plt.text((l_min + l_max)/2, np.sin(l_min) - (yax[1]-yax[0]) / 20, \"$\\Delta x$\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print('slope =', (np.sin(l_max) - np.sin(l_min)) / (l_max - l_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8CYa2CRpP9N"
   },
   "source": [
    "Видим, что при уменьшении отрезка $\\Delta x$, значение приращения стабилизируется (перестаёт изменяться). Это число и есть приращение функции в точке, равное проиводной функции в точке. Производную функции $f(x)$ в точке x обознают как $f'(x)$ или как $\\frac{d}{dx}(f(x))$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VMwBqnhVpP9N"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Пример вычисления проиводной</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JwlAAsznpP9P"
   },
   "source": [
    "Возьмём производную по определению:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6R_rnMsqpP9P"
   },
   "source": [
    "1. $f(x)=x$  \n",
    "\n",
    "$$\\frac{\\Delta y}{\\Delta x}=\\frac{x+\\Delta x-x}{\\Delta x}=1\\Rightarrow \\mathbf{\\frac{d}{dx}(x)=1}$$  \n",
    "\n",
    "2. $f(x)=x^2$  \n",
    "\n",
    "$$\\frac{\\Delta y}{\\Delta x}=\\frac{(x+\\Delta x)^2-x^2}{\\Delta x}=\\frac{x^2+2x\\Delta x+\\Delta x^2-x^2}{\\Delta x}=2x+\\Delta x\\rightarrow 2x (\\Delta x\\rightarrow 0)\\Rightarrow \\mathbf{\\frac{d}{dx}(x^2)=2x}$$  \n",
    "    \n",
    "3. В общем случае для степенной функции $f(x)=x^n$ формула будет такой:  \n",
    "\n",
    "$$\\mathbf{\\frac{d}{dx}(x^n)=nx^{n-1}}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KP4jUOaqpP9P"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Правила вычисления проиводной</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8fb0go1lpP9Q"
   },
   "source": [
    "Выпишет правила *дифференцирования*:  \n",
    "\n",
    "1). Если $f(x)$ - константа, то её производная (приращение) 0:  \n",
    "\n",
    "$$(C)' = 0$$\n",
    "\n",
    "2). Производная суммы функций - это сумма производных:  \n",
    "\n",
    "$$(f(x) + g(x))' = f'(x) + g'(x)$$\n",
    "\n",
    "3). Производная разности - разность производных:  \n",
    "\n",
    "$$(f(x) - g(x))' = f'(x) - g'(x)$$\n",
    "\n",
    "4). Производная произведения функций:  \n",
    "\n",
    "$$(f(x)g(x))' = f'(x)g(x) + f(x)g'(x)$$\n",
    "\n",
    "5). Производная частного:  \n",
    "\n",
    "$$\\left(\\frac{f(x)}{g(x)}\\right)'=\\frac{f'(x)g(x)-g'(x)f(x)}{g^2(x)}$$\n",
    "\n",
    "6). Производная сложной функции (\"правило цепочки\", \"chain rule\"):  \n",
    "\n",
    "$$(f(g(x)))'=f'(g(x))g'(x)$$\n",
    "\n",
    "Можно записать ещё так:  \n",
    "\n",
    "$$\\frac{d}{dx}(f(g(x)))=\\frac{df}{dg}\\frac{dg}{dx}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZFTReW5ppP9R"
   },
   "source": [
    "**Примеры**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "grjqr2h4pP9R"
   },
   "source": [
    "* Вычислим производную функции $$f(x) = \\frac{x^2}{cos(x)} + 100$$:  \n",
    "\n",
    "$$f'(x) = \\left(\\frac{x^2}{cos(x)}+100\\right)' = \\left(\\frac{x^2}{cos(x)}\\right)' + (100)' = \\frac{(2x)\\cos(x) - x^2(-\\sin(x))}{cos^2(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TSqCaSSYpP9T"
   },
   "source": [
    "* Вычислим производную функции $$f(x) = tg(x)$$:  \n",
    "\n",
    "$$f'(x) = \\left(tg(x)\\right)' = \\left(\\frac{\\sin(x)}{\\cos(x)}\\right)' = \\frac{\\cos(x)\\cos(x) - \\sin(x)(-\\sin(x))}{cos^2(x)} = \\frac{1}{cos^2(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXP_YETzpP9T"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Частные производные</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJDwZBCZpP9T"
   },
   "source": [
    "Когда мы имеем функци многих переменных, её уже сложнее представить себе в виде рисунка (в случае более 3-х переменных это действительно не всем дано). ОДнако формальные правила взятия производной у таких функций созраняются. Они в точности совпадают с тоеми, которые рассмотрены выше для функции одной переменной.  \n",
    "\n",
    "Итак, правило взятия частной производной функции мнгих переменных:  \n",
    "1). Пусть $f(\\overline{x}) = f(x_1, x_2, .., x_n)$ - функция многих переменных;  \n",
    "2). Частная проиводная по $x_i$ это функции - это производная по x_i, считая все остальные переменные **константами**. \n",
    "\n",
    "Более математично:  \n",
    "\n",
    "Частная производная функции $f(x_1,x_2,...,x_n)$ по $x_i$ равна  \n",
    "\n",
    "$$\\frac{\\partial f(x_1,x_2,...,x_n)}{\\partial x_i}=\\frac{df_{x_1,...,x_{i-1},x_{i+1},...x_n}(x_i)}{dx_i}$$  \n",
    "\n",
    "где $f_{x_1,...,x_{i-1},x_{i+1},...x_n}(x_i)$ означает, что переменные $x_1,...,x_{i-1},x_{i+1},...x_n$ - это фиксированные значения, и с ними нужно обращаться как с константами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3uPIkZ-wpP9U"
   },
   "source": [
    "**Примеры**:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aodpt9VppP9V"
   },
   "source": [
    "* Найдём частные производные функции $f(x, y) = -x^7 + (y - 2)^2 + 140$ по $x$ и по $y$:  \n",
    "\n",
    "$$f_x'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{x}} = -7x^6$$  \n",
    "$$f_y'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{y}} = 2(y - 2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3pRitR-YpP9W"
   },
   "source": [
    "* Найдём частные производные функции $f(x, y, z) = \\sin(x)\\cos(y)tg(z)$ по $x$, по $y$ и по $z$:  \n",
    "\n",
    "$$f_x'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{x}} = \\cos(x)\\cos(y)tg(z)$$  \n",
    "$$f_y'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{y}} = \\sin(x)(-\\sin(y))tg(z)$$\n",
    "$$f_z'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{y}} = \\frac{\\sin(x)\\cos(y)}{\\cos^2{z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nrmPlYyrpP9X"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Градиентный спуск</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SeD8U4CApP9X"
   },
   "source": [
    "**Градиентом** функции $f(\\overline{x})$, где $\\overline{x} \\in \\mathbb{R^n}$, то есть $\\overline{x} = (x_1, x_2, .., x_n)$, называется вектор из частных производных функции $f(\\overline{x})$:  \n",
    "\n",
    "$$grad(f) = \\nabla f(\\overline{x}) = \\left(\\frac{\\partial{f(\\overline{x})}}{\\partial{x_1}}, \\frac{\\partial{f(\\overline{x})}}{\\partial{x_2}}, .., \\frac{\\partial{f(\\overline{x})}}{\\partial{x_n}}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DcjYCHOepP9Y"
   },
   "source": [
    "Есть функция $f(x)$. Хотим найти аргумент, при котором она даёт минимум.\n",
    "\n",
    "Алгоритм градиентного спуска:  \n",
    "1. $x^0$ - начальное значение (обычно берётся просто из разумных соображений или случайное);  \n",
    "2. $x^i = x^{i-1} - \\alpha \\nabla f(x^{i-1})$, где $\\nabla f(x^{i-1})$ - это градиент функции $f$, в который подставлено значение $x^{i-1}$;\n",
    "3. Выполнять пункт 2, пока не выполнится условие остановки: $||x^{i} - x^{i-1}|| < eps$, где $||x^{i} - x^{i-1}|| = \\sqrt{(x_1^i - x_1^{i-1})^2 + .. + (x_n^i - x_n^{i-1})^2}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zX1miuQ0pP9Z"
   },
   "source": [
    "**Примеры:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t1M6agxdpP9Z"
   },
   "source": [
    "* *Пример 1*: Посчитаем формулу градиентного спуска для функции $f(x) = 10x^2$:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1WMJRqDRpP9a"
   },
   "source": [
    "$x^i = x^{i-1} - \\alpha \\nabla f(x^{i-1}) = x^{i-1} - \\alpha f'(x^{i-1}) = x^{i-1} - \\alpha (20x^{i-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EqjopRZVpP9b"
   },
   "source": [
    "Имея эту формулу, напишем код градиентного спуска для функции $f(x) = 10x^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "evLahkyIpP9c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def f(x):\n",
    "    return 10 * x**2\n",
    "\n",
    "def gradient_descent(alpha=0.01, eps=0.001):\n",
    "    x_pred = 100  # начальная инициализация\n",
    "    x = 50  # начальная инициализация\n",
    "    for _ in tqdm(range(100000)):\n",
    "        print(\"Step:\",_,\"\\tX =\", round(x,5),\"\\tf(X) =\", round(f(x), 5))  # смотрим, на каком мы шаге\n",
    "        if np.sum((x - x_pred)**2) < eps**2:  # условие остановки\n",
    "            break\n",
    "        x_pred = x\n",
    "        x = x_pred - 20 * alpha * x_pred  # по формуле выше\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 501,
     "status": "ok",
     "timestamp": 1539764935297,
     "user": {
      "displayName": "Григорий Лелейтнер",
      "photoUrl": "",
      "userId": "07179937308049589303"
     },
     "user_tz": -180
    },
    "id": "9k8A7ei8pP9g",
    "outputId": "7d0401a9-5810-4215-89af-4a9d077c05eb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 \tX = 50 \tf(X) = 25000\n",
      "Step: 1 \tX = 40.0 \tf(X) = 16000.0\n",
      "Step: 2 \tX = 32.0 \tf(X) = 10240.0\n",
      "Step: 3 \tX = 25.6 \tf(X) = 6553.6\n",
      "Step: 4 \tX = 20.48 \tf(X) = 4194.304\n",
      "Step: 5 \tX = 16.384 \tf(X) = 2684.35456\n",
      "Step: 6 \tX = 13.1072 \tf(X) = 1717.98692\n",
      "Step: 7 \tX = 10.48576 \tf(X) = 1099.51163\n",
      "Step: 8 \tX = 8.38861 \tf(X) = 703.68744\n",
      "Step: 9 \tX = 6.71089 \tf(X) = 450.35996\n",
      "Step: 10 \tX = 5.36871 \tf(X) = 288.23038\n",
      "Step: 11 \tX = 4.29497 \tf(X) = 184.46744\n",
      "Step: 12 \tX = 3.43597 \tf(X) = 118.05916\n",
      "Step: 13 \tX = 2.74878 \tf(X) = 75.55786\n",
      "Step: 14 \tX = 2.19902 \tf(X) = 48.35703\n",
      "Step: 15 \tX = 1.75922 \tf(X) = 30.9485\n",
      "Step: 16 \tX = 1.40737 \tf(X) = 19.80704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 17/100000 [00:00<10:25, 159.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 17 \tX = 1.1259 \tf(X) = 12.67651\n",
      "Step: 18 \tX = 0.90072 \tf(X) = 8.11296\n",
      "Step: 19 \tX = 0.72058 \tf(X) = 5.1923\n",
      "Step: 20 \tX = 0.57646 \tf(X) = 3.32307\n",
      "Step: 21 \tX = 0.46117 \tf(X) = 2.12676\n",
      "Step: 22 \tX = 0.36893 \tf(X) = 1.36113\n",
      "Step: 23 \tX = 0.29515 \tf(X) = 0.87112\n",
      "Step: 24 \tX = 0.23612 \tf(X) = 0.55752\n",
      "Step: 25 \tX = 0.18889 \tf(X) = 0.35681\n",
      "Step: 26 \tX = 0.15112 \tf(X) = 0.22836\n",
      "Step: 27 \tX = 0.12089 \tf(X) = 0.14615\n",
      "Step: 28 \tX = 0.09671 \tf(X) = 0.09354\n",
      "Step: 29 \tX = 0.07737 \tf(X) = 0.05986\n",
      "Step: 30 \tX = 0.0619 \tf(X) = 0.03831\n",
      "Step: 31 \tX = 0.04952 \tf(X) = 0.02452\n",
      "Step: 32 \tX = 0.03961 \tf(X) = 0.01569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_min = gradient_descent(0.01, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1539764937543,
     "user": {
      "displayName": "Григорий Лелейтнер",
      "photoUrl": "",
      "userId": "07179937308049589303"
     },
     "user_tz": -180
    },
    "id": "uGOVAybRpP9k",
    "outputId": "2c6d1f12-ce1e-491f-ca98-6e1fe98849c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03961408125713218"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 598,
     "status": "ok",
     "timestamp": 1539764938543,
     "user": {
      "displayName": "Григорий Лелейтнер",
      "photoUrl": "",
      "userId": "07179937308049589303"
     },
     "user_tz": -180
    },
    "id": "_EW8AY8VpP9n",
    "outputId": "4d7b79da-706d-4d04-c1ba-29d39625760f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01569275433846671"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tsqfsTezpP9q"
   },
   "source": [
    "* *Пример 2*: Посчитаем формулу градиентного спуска для функции $f(x, y) = 10x^2 + y^2$:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MNWMBcbNpP9r"
   },
   "source": [
    "$$\\left(\\begin{matrix} x^i \\\\ y^i \\end{matrix}\\right) = \\left(\\begin{matrix} x^{i-1} \\\\ y^{i-1} \\end{matrix}\\right) - \\alpha \\nabla f(x^{i-1}, y^{i-1}) = \\left(\\begin{matrix} x^{i-1} \\\\ y^{i-1} \\end{matrix}\\right) - \\alpha \\left(\\begin{matrix} \\frac{\\partial{f(x^{i-1}, y^{i-1})}}{\\partial{x}} \\\\ \\frac{\\partial{f(x^{i-1}, y^{i-1})}}{\\partial{y}} \\end{matrix}\\right) = x^{i-1} - \\alpha \\left(\\begin{matrix} 20x^{i-1} \\\\ 2y^{i-1} \\end{matrix}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sBnijsKLpP9r"
   },
   "source": [
    "Осталось написать код, выполняющий градиентный спуск, пока не выполнится условие остановки, для функции $f(x, y) = 10x^2 + y^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_rDsja-pP9s"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def f(x):\n",
    "    return 10 * x[0]**2 + x[1]**2\n",
    "\n",
    "def gradient_descent(alpha=0.01, eps=0.001):\n",
    "    x_prev = np.array([100, 100])  # начальная инициализация\n",
    "    x = np.array([50, 50])  # начальная инициализация\n",
    "    for _ in tqdm(range(100000)):\n",
    "        print(\"Step:\",_,\"\\tX =\", np.round(x,3),\"\\tf(X) =\", np.round(f(x), 5))  # смотрим, на каком мы шаге\n",
    "        if np.sum((x - x_prev)**2) < eps**2:  # условие остановки\n",
    "            break\n",
    "        x_prev = x\n",
    "        x = x_prev - alpha * np.array(20 * x_prev[0], 2 * x_prev[1])  # по формуле выше\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "boueQCnXpP9u",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 \tX = [50 50] \tf(X) = 27500\n",
      "Step: 1 \tX = [40. 40.] \tf(X) = 17600.0\n",
      "Step: 2 \tX = [32. 32.] \tf(X) = 11264.0\n",
      "Step: 3 \tX = [25.6 25.6] \tf(X) = 7208.96\n",
      "Step: 4 \tX = [20.48 20.48] \tf(X) = 4613.7344\n",
      "Step: 5 \tX = [16.384 16.384] \tf(X) = 2952.79002\n",
      "Step: 6 \tX = [13.107 13.107] \tf(X) = 1889.78561\n",
      "Step: 7 \tX = [10.486 10.486] \tf(X) = 1209.46279\n",
      "Step: 8 \tX = [8.389 8.389] \tf(X) = 774.05619\n",
      "Step: 9 \tX = [6.711 6.711] \tf(X) = 495.39596\n",
      "Step: 10 \tX = [5.369 5.369] \tf(X) = 317.05341\n",
      "Step: 11 \tX = [4.295 4.295] \tf(X) = 202.91418\n",
      "Step: 12 \tX = [3.436 3.436] \tf(X) = 129.86508\n",
      "Step: 13 \tX = [2.749 2.749] \tf(X) = 83.11365\n",
      "Step: 14 \tX = [2.199 2.199] \tf(X) = 53.19274\n",
      "Step: 15 \tX = [1.759 1.759] \tf(X) = 34.04335\n",
      "Step: 16 \tX = [1.407 1.407] \tf(X) = 21.78774\n",
      "Step: 17 \tX = [1.126 1.126] \tf(X) = 13.94416\n",
      "Step: 18 \tX = [0.901 0.901] \tf(X) = 8.92426\n",
      "Step: 19 \tX = [0.721 0.721] \tf(X) = 5.71153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 20/100000 [00:00<08:38, 192.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 20 \tX = [0.576 0.576] \tf(X) = 3.65538\n",
      "Step: 21 \tX = [0.461 0.461] \tf(X) = 2.33944\n",
      "Step: 22 \tX = [0.369 0.369] \tf(X) = 1.49724\n",
      "Step: 23 \tX = [0.295 0.295] \tf(X) = 0.95824\n",
      "Step: 24 \tX = [0.236 0.236] \tf(X) = 0.61327\n",
      "Step: 25 \tX = [0.189 0.189] \tf(X) = 0.39249\n",
      "Step: 26 \tX = [0.151 0.151] \tf(X) = 0.2512\n",
      "Step: 27 \tX = [0.121 0.121] \tf(X) = 0.16077\n",
      "Step: 28 \tX = [0.097 0.097] \tf(X) = 0.10289\n",
      "Step: 29 \tX = [0.077 0.077] \tf(X) = 0.06585\n",
      "Step: 30 \tX = [0.062 0.062] \tf(X) = 0.04214\n",
      "Step: 31 \tX = [0.05 0.05] \tf(X) = 0.02697\n",
      "Step: 32 \tX = [0.04 0.04] \tf(X) = 0.01726\n",
      "Step: 33 \tX = [0.032 0.032] \tf(X) = 0.01105\n",
      "Step: 34 \tX = [0.025 0.025] \tf(X) = 0.00707\n",
      "Step: 35 \tX = [0.02 0.02] \tf(X) = 0.00453\n",
      "Step: 36 \tX = [0.016 0.016] \tf(X) = 0.0029\n",
      "Step: 37 \tX = [0.013 0.013] \tf(X) = 0.00185\n",
      "Step: 38 \tX = [0.01 0.01] \tf(X) = 0.00119\n",
      "Step: 39 \tX = [0.008 0.008] \tf(X) = 0.00076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 40/100000 [00:00<09:03, 183.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 40 \tX = [0.007 0.007] \tf(X) = 0.00049\n",
      "Step: 41 \tX = [0.005 0.005] \tf(X) = 0.00031\n",
      "Step: 42 \tX = [0.004 0.004] \tf(X) = 0.0002\n",
      "Step: 43 \tX = [0.003 0.003] \tf(X) = 0.00013\n",
      "Step: 44 \tX = [0.003 0.003] \tf(X) = 8e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_min = gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6pyhQsmXpP9x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00272226, 0.00272226])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ytAfn_X7pP90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.151763082307056e-05"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YyKaCWuJpP93"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Домашнее задание</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jrjiC9mUpP93"
   },
   "source": [
    "1). (только для тех, кто раньше брал производные) Вычислите производную функции $f(x)=\\frac{1}{x}$ по определению и сравните с производной степенной функции в общем случае;  \n",
    "2). Найдите производную функции $Cf(x)$, где С - число;  \n",
    "3). Найдите производные функций:  \n",
    "\n",
    "$$f(x)=x^3+3\\sqrt{x}-e^x$$\n",
    "\n",
    "$$f(x)=\\frac{x^2-1}{x^2+1}$$\n",
    "\n",
    "$$\\sigma(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "$$L(y, \\hat{y}) = (y-\\hat{y})^2$$  \n",
    "\n",
    "4). Напишите формулу и код для градиентного спуска для функции:  \n",
    "$$f(w, x) = \\frac{1}{1 + e^{-wx}}$$  \n",
    "\n",
    "То есть по аналогии с примером 2 вычислите частные производные по $w$ и по $x$ и запишите формулу векторно (см. пример 2)\n",
    "\n",
    "В задаче 3 производную нужно брать по $\\hat{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxDBOB04pP93"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Полезные ссылки</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nm0VC825pP95"
   },
   "source": [
    "0). Прикольный сайт с рисунками путём задания кривых уравнениями и функциями:  \n",
    "\n",
    "https://www.desmos.com/calculator/jwshvscdzb\n",
    "\n",
    "***Производные:***\n",
    "\n",
    "1). Про то, как брать частные производные:  \n",
    "\n",
    "http://www.mathprofi.ru/chastnye_proizvodnye_primery.html\n",
    "\n",
    "2). Сайт на английском, но там много видеоуроков и задач по производным:  \n",
    "\n",
    "https://www.khanacademy.org/math/differential-calculus/derivative-intro-dc\n",
    "\n",
    "3). Задачи на частные производные:  \n",
    "\n",
    "http://ru.solverbook.com/primery-reshenij/primery-resheniya-chastnyx-proizvodnyx/  \n",
    "\n",
    "4). Ещё задачи на частные проивзодные:  \n",
    "\n",
    "https://xn--24-6kcaa2awqnc8dd.xn--p1ai/chastnye-proizvodnye-funkcii.html  \n",
    "\n",
    "5). Производные по матрицам:  \n",
    "\n",
    "http://nabatchikov.com/blog/view/matrix_der  \n",
    "\n",
    "***Градиентны спуск:***\n",
    "\n",
    "6). [Основная статья по градиентному спуску](http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D0%BE%D0%B3%D0%BE_%D1%81%D0%BF%D1%83%D1%81%D0%BA%D0%B0)\n",
    "\n",
    "7). Статья на Хабре про градиетный спуск для нейросетей:  \n",
    "\n",
    "https://habr.com/post/307312/  \n",
    "\n",
    "***Методы оптимизации в нейронных сетях:***\n",
    "\n",
    "8). Сайт с анимациями того, как сходятся алгоритмы градиентного спуска:\n",
    "www.denizyuret.com/2015/03/alec-radfords-animations-for.html\n",
    "\n",
    "9). Статья на Хабре про метопты (град. спуск) в нейронках:\n",
    "https://habr.com/post/318970/\n",
    "\n",
    "10). Ещё сайт (англ.) про метопты (град. спуск) в нейронках (очень подробно):\n",
    "http://ruder.io/optimizing-gradient-descent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[seminar]derivative_gradient.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
