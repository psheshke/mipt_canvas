{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EvkV_R3vzl_m"
   },
   "source": [
    "<p style=\"align: center;\"><img align=center src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\"  width=400></p>\n",
    "\n",
    "<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rbRbOixxzl_n"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4mYrykelzl_o"
   },
   "source": [
    "<h2 style=\"text-align: center;\"><b>Градиентный спуск и линейные модели: домашнее задание</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K1xoftyezl_p"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from matplotlib import pylab, gridspec, pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yY2kUgovzl_s"
   },
   "source": [
    "### 1. Градиентный спуск: повторение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ctYan0Tfzl_t"
   },
   "source": [
    "Рассмотрим функцию от двух переменных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xDCA6t7Azl_v"
   },
   "outputs": [],
   "source": [
    "def f(x1, x2):\n",
    "    return np.sin(x1)**2 + np.sin(x2) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uV1aB-CQzl_w"
   },
   "source": [
    "***Reminder:***  \n",
    "Что мы хотим? Мы хотим найти минимум этой функции (в машинному обучении мы обычно хотим найти минимум **функции потерь**, например, MSE), а точнее найти x1 и x2 такие, что при них значение f(x1,x2) минимально, то есть *точку экстремума*.  \n",
    "Как мы будем искать эту точку? Используем методы оптимизации (=минимизации в нашем случае). Одним из таких методов и является градиентный спуск. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7b185-dEzl_x"
   },
   "source": [
    "Реализуем функцию, которая будет осуществлять градиентный спуск для функции f:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lsnvRASDkYXi"
   },
   "source": [
    "*Примечание:* Вам нужно посчитать частные производные именно **аналитически** и **переписать их в код**, а не считать производные численно (через отношение приращения функции к приращению аргумента) -- в этих двух случаях могут различаться ответы, поэтому будьте внимательны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VfgA69Vizl_y"
   },
   "outputs": [],
   "source": [
    "def grad_descent(lr, num_iter=100):\n",
    "    \"\"\"\n",
    "    функция, которая реализует градиентный спуск в минимум для функции f от двух переменных. \n",
    "        param lr: learning rate алгоритма\n",
    "        param num_iter: количество итераций градиентного спуска\n",
    "    \"\"\"\n",
    "    global f\n",
    "    # в начале градиентного спуска инициализируем значения x1 и x2 какими-нибудь числами\n",
    "    cur_x1, cur_x2 = 1.5, -1\n",
    "    # будем сохранять значения аргументов и значений функции в процессе град. спуска в переменную states\n",
    "    steps = []\n",
    "    \n",
    "    # итерация цикла -- шаг градиентнго спуска\n",
    "    for iter_num in range(num_iter):\n",
    "        steps.append([cur_x1, cur_x2, f(cur_x1, cur_x2)])\n",
    "        \n",
    "        # чтобы обновить значения cur_x1 и cur_x2, как мы помним с последнего занятия, \n",
    "        # нужно найти производные (градиенты) функции f по этим переменным.\n",
    "        grad_x1 = <тут Ваш код>\n",
    "        grad_x2 = <тут Ваш код>\n",
    "                 \n",
    "        # после того, как посчитаны производные, можно обновить веса. \n",
    "        # не забудьте про lr!\n",
    "        cur_x1 -= <тут Ваш код>\n",
    "        cur_x2 -= <тут Ваш код>\n",
    "    return np.array(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JKffGXNrzl_0"
   },
   "source": [
    "Запустим градиентный спуск:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XVtijv6Dzl_1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "steps = grad_descent(lr=0.5, num_iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GKaPajd6zl_4"
   },
   "source": [
    "Визуализируем точки градиентного спуска на 3D-графике нашей функции. Звездочками будут обозначены точки (тройки x1, x2, f(x1, x2)), по которым Ваш алгоритм градиентного спуска двигался к минимуму.\n",
    "\n",
    "(Для того, чтобы написовать этот график, мы и сохраняли значения cur_x1, cur_x2, f(cur_x1, cur_x2) в steps в процессе спуска)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JODF87Ilzl_4"
   },
   "source": [
    "Если у Вас правильно написана функция grad_descent, то звездочки на картинке должны сходиться к одной из точку минимума функции. Вы можете менять начальные приближения алгоритма, значения lr и num_iter и получать разные результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y2PqLNfEzl_5"
   },
   "outputs": [],
   "source": [
    "# %matplotlib osx\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "\n",
    "path = []\n",
    "\n",
    "X, Y = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "ax.plot(xs=steps[:, 0], ys=steps[:, 1], zs=steps[:, 2], marker='*', markersize=20,\n",
    "                markerfacecolor='y', lw=3, c='black')\n",
    "\n",
    "ax.plot_surface(X, Y, f(X, Y), cmap=cm.coolwarm)\n",
    "ax.set_zlim(0, 5)\n",
    "ax.view_init(elev=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwaDZNzzzl_8"
   },
   "source": [
    "#### Ответ на КР  №1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0PoHtXI4zl_9"
   },
   "source": [
    "Запустите Вашу функцию grad_descent c параметрами lr=0.3, num_iter=20 и начальными приближениями  cur_x1, cur_x2 = 1.5, -1. Сумма значений x1, x2 и f(x1, x2) (т.е. сумма значений в steps[-1]), умноженная на 10\\**6 и округленная до 2 знаков после запятой, будет ответом на первый вопрос кр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IMOAWHEBzl_-"
   },
   "outputs": [],
   "source": [
    "steps = grad_descent(<тут Ваш код>)\n",
    "np.sum(steps[-1]) * 10**6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vvgaD86KzmAC"
   },
   "source": [
    "### 2. Линейные модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQ50iMzwzmAC"
   },
   "source": [
    "Возьмем код для линейной регресси с семинара. Напомним, что найти веса W и b для линейной регресси можно двумя способами: обращением матриц (функция solve_weights) и градиентным спуском (функция grad_descent). Мы здесь будем рассматривать градиентный спуск."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Mm7F1rizmAD"
   },
   "outputs": [],
   "source": [
    "W = None\n",
    "b = None\n",
    "\n",
    "def mse(preds, y):\n",
    "    return ((preds - y)**2).mean()\n",
    "    \n",
    "def grad_descent(X, y, lr, num_iter=100):\n",
    "    global W, b\n",
    "    np.random.seed(40)\n",
    "    W = np.random.rand(X.shape[1])\n",
    "    b = np.array(np.random.rand(1))\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    for iter_num in range(num_iter):\n",
    "        preds = predict(X)\n",
    "        losses.append(mse(preds, y))\n",
    "        \n",
    "        w_grad = np.zeros_like(W)\n",
    "        b_grad = 0\n",
    "        for sample, prediction, label in zip(X, preds, y):\n",
    "            w_grad += 2 * (prediction - label) * sample\n",
    "            b_grad += 2 * (prediction - label)\n",
    "            \n",
    "        W -= lr * w_grad\n",
    "        b -= lr * b_grad\n",
    "    return losses\n",
    "\n",
    "def predict(X):\n",
    "    global W, b\n",
    "    return np.squeeze(X @ W + b.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYKGc0aHzmAG"
   },
   "source": [
    "Рассмотрим функцию:  \n",
    "\n",
    "$$f(x, y) = 0.43x+0.5y + 0.67$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kQXFf_Y9zmAH"
   },
   "source": [
    "***Friendly reminder:***  \n",
    "Что мы хотим? Мы хотим уметь \"восстанавливать функцию\" -- то есть предсказывать значения функции в точках с координатами (x, y) (именно так и получается 3D-график -- (x, y, f(x,y)) в пространстве).  \n",
    "В чём сложность? Нам дан только конечный небольшой набор точек (30 в данном случае), по которому мы хотим восстановить зависимость, по сути, непрерывную. Линейная регрессия как раз подходит для восстановления линейной зависимости (а у нас функция сейчас как раз линейная -- только сложение аргументов и умножение на число)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sYO-jgNszmAK"
   },
   "source": [
    "Cгерерируем шумные данные из этой функции (как на семинаре):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qKjm2cQrzmAK"
   },
   "outputs": [],
   "source": [
    "np.random.seed(40)\n",
    "func = lambda x, y: (0.43*x + 0.5*y + 0.67 + np.random.normal(0, 7, size=x.shape))\n",
    "\n",
    "X = np.random.sample(size=(30)) * 10\n",
    "Y = np.random.sample(size=(30)) * 150\n",
    "result_train = [func(x, y) for x, y in zip(X, Y)]\n",
    "data_train = np.concatenate([X.reshape(-1, 1), Y.reshape(-1, 1)], axis=1)\n",
    "\n",
    "pd.DataFrame({'x': X, 'y': Y, 'res': result_train}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ETB66yWyzmAN"
   },
   "source": [
    "Посмотрим, что же мы сгенерировали:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DUXvT674zmAN"
   },
   "outputs": [],
   "source": [
    "# %matplotlib osx\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "ax = fig.gca(projection='3d')\n",
    "X, Y = np.meshgrid(np.linspace(0, 10, 100), np.linspace(0, 150, 100))\n",
    "ax.scatter(xs=data_train[:, 0], ys=data_train[:, 1], zs=result_train, c='r')\n",
    "ax.plot_surface(X,Y, 0.43*X + 0.5*Y + 0.67, color='green', alpha=0.3)\n",
    "\n",
    "ax.view_init(elev=60)\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "09leW_BazmAQ"
   },
   "source": [
    "Теперь давайте попробуем применить к этим данным нашу линейную регрессию и с помощью неё предсказать истинный график функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XXYMvQxfzmAR"
   },
   "outputs": [],
   "source": [
    "losses = grad_descent(data_train, result_train, 1e-2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UITmWCUNzmAT"
   },
   "outputs": [],
   "source": [
    "W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P8fA0-DczmAV"
   },
   "source": [
    "Посмотрим на график лосса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_LPNor1fzmAW"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses), losses[-1];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sVYB8qUOzmAZ"
   },
   "source": [
    "И на полученную разделяющую плоскость:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gMdpNJM6zmAa"
   },
   "outputs": [],
   "source": [
    "# %matplotlib osx\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "ax = fig.gca(projection='3d')\n",
    "X, Y = np.meshgrid(np.linspace(0, 10, 100), np.linspace(0, 150, 100))\n",
    "ax.scatter(xs=data_train[:, 0], ys=data_train[:, 1], zs=result_train, c='r')\n",
    "ax.plot_surface(X,Y, 0.43*X + 0.5*Y + 0.67, color='green', alpha=0.3)\n",
    "ax.plot_surface(X,Y, W[0]*X + W[1]*Y + b, color='blue', alpha=0.3)\n",
    "\n",
    "ax.view_init(elev=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sFczLgyuzmAe"
   },
   "source": [
    "(зелёная плоскость -- истинная функция, синяя плоскость -- предсказание)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0BzdivVFzmAf"
   },
   "source": [
    "Охх, лосс и коэффициенты (W и b) нашей модели быстро уходит в небеса, и график предсказан неправильно. Почему такое происходит?\n",
    "\n",
    "В данном случае дело в том, что признаки имеют разный *масштаб* (посмотрите на значения X и Y -- они лежат в разных диапазонах). Многие модели машинного обучения, в том числе линейные, будут плохо работать в таком случае (на самом деле это зависит от метода оптимизации, сейчас это градиентный спуск).  \n",
    "\n",
    "Есть несколько способов **масштабирования**:\n",
    "1. **Нормализация (она же стандартизация, StandardScaling)**:  \n",
    "\n",
    "$$x_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j}$$  \n",
    "(j -- номер признака, i -- номер объекта)  \n",
    "То есть вычитаем среднее по столбцу и делим на корень из дисперсии.\n",
    "\n",
    "2. **Приведение к отрезку [0,1] (MinMaxScaling)**:  \n",
    "\n",
    "$$x_{ij} = \\frac{x_{ij} - \\min_j}{\\max_j - \\min_j}$$  \n",
    "(j -- номер признака, i -- номер объекта)  \n",
    "То есть вычитаем минимум по столбцу и делим на разницу между минимумом и максимумом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oOXgLIWTzmAg"
   },
   "source": [
    "Используем нормализацию:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4gOqi21ezmAh"
   },
   "source": [
    "### 2.1 Нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XfoQbSEyzmAi"
   },
   "source": [
    "Посмотрим на среднее и разброс значений в признаках (координатах) до масштабирования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "npJ9aGzZzmAi"
   },
   "outputs": [],
   "source": [
    "data_train.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SghPwBF3zmAk"
   },
   "outputs": [],
   "source": [
    "data_train.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZcvJRf1zmAm"
   },
   "source": [
    "То есть в первом столбце у нас среднее 5.6 и среднеквадратичное отклонение 2.7, а во втором столбце среднее 70.8 и среднеквадратичное отклонение 45.3.  \n",
    "\n",
    "Будьте внимательны: среднеквадратичное отклонение НЕ говорит о том, какой *максимальный* разброс, оно лишь указывает на числовой интервал, вероятность попадания в который у данного признака высока (то есть, например, в интервал [2.9, 8.3] в первом случае). Для большей информации смотрите [среднеквадратичное отклонение](https://ru.wikipedia.org/wiki/%D0%A1%D1%80%D0%B5%D0%B4%D0%BD%D0%B5%D0%BA%D0%B2%D0%B0%D0%B4%D1%80%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5_%D0%BE%D1%82%D0%BA%D0%BB%D0%BE%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5) и [доверительные интервалы](https://ru.wikipedia.org/wiki/%D0%94%D0%BE%D0%B2%D0%B5%D1%80%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B9_%D0%B8%D0%BD%D1%82%D0%B5%D1%80%D0%B2%D0%B0%D0%BB)\n",
    "\n",
    "Однако факт в том, что масштаб у этих признаков разный, давайте исправим это."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I0DMbY5yzmAn"
   },
   "source": [
    "**Нормализуйте признаки** так, чтобы среднее значение в каждом столбце было ~0, а стандартное отклонение ~1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "by_okjU0zmAo"
   },
   "outputs": [],
   "source": [
    "data_train_normalized = <тут Ваш код>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hp-JLFDRzmAr"
   },
   "source": [
    "Проверьте средние и диспресию (.std()):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUFuFHrmzmAs"
   },
   "outputs": [],
   "source": [
    "<тут Ваш код>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cOa4B4IXzmAu"
   },
   "outputs": [],
   "source": [
    "<тут Ваш код>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmFr422fzmAy"
   },
   "source": [
    "Попробуем теперь запустить регрессию с теми же параметрами lr и num_iter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0QbFEV7ZzmAy"
   },
   "outputs": [],
   "source": [
    "losses = grad_descent(data_train_normalized, result_train, 1e-2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tVruzNtAzmA0"
   },
   "outputs": [],
   "source": [
    "W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vn4NXpo4zmA2"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "print(losses[-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "de8U-uSYzmA4"
   },
   "source": [
    "Мы видим, что теперь коэффициенты по модулю не огромны, градиентный спуск не взрывается и лосс стабилен!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-wLnnMekzmA5"
   },
   "source": [
    "Посмотрим на полученную плоскость:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZGEhS_nvzmA5"
   },
   "outputs": [],
   "source": [
    "# %matplotlib osx\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "ax.scatter(xs=data_train[:, 0], ys=data_train[:, 1], zs=result_train, c='r')\n",
    "\n",
    "X, Y = np.meshgrid(np.linspace(-1, 10, 100), np.linspace(-1, 150, 100))\n",
    "ax.plot_surface(X, Y, 0.43*X + 0.5*Y + 0.67, color='green', alpha=0.3)\n",
    "\n",
    "X, Y = np.meshgrid(np.linspace(-1, 10, 100), np.linspace(-1, 150, 100))\n",
    "# Hint: не забудьте нормализовать X и Y тоже\n",
    "ax.plot_surface(X, Y, W[0] * <тут Ваш код> + W[1] * <тут Ваш код> + b, color='blue', alpha=0.3)\n",
    "\n",
    "ax.view_init(elev=60)\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UVR-nmjszmA7"
   },
   "source": [
    "#### Ответ на КР  №2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H2j4xKJ3zmA8"
   },
   "source": [
    "Запустите градиентный спуск на нормализованных данных с параметрами lr=1e-2, num_iter=200 и посчитайте сумму последних трех значений лосса. Полученное число, округленное до второго знака после запятой, будет ответом на второе задание кр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n-R6jefyzmA8"
   },
   "outputs": [],
   "source": [
    "losses = grad_descent(<тут Ваш код>)\n",
    "np.sum(losses[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LnJmxSlCzmA-"
   },
   "source": [
    "### 2.2 Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0NXw5MtzmA_"
   },
   "source": [
    "Помимо \"сырой\" линейной регресси часто используют линейную регрессию с регуляризацией -- Lasso или Ridge регрессию. Они отличаются только типом \"штрафа\" за большие веса: учитывать модули (Lasso) или квадраты весов (Ridge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "99ajXzw8zmA_"
   },
   "source": [
    "Учитывая, что наш лосс в этой задаче -- Mean Squared Error (MSE), в случае Ridge-регресси будет:  \n",
    "\n",
    "$$Loss = MSE = \\sum (pred_i-y_i)^2 + \\alpha*\\sum W_i^2$$  \n",
    "\n",
    "А в случае Lasso-регресси будет:  \n",
    "\n",
    "$$Loss = MSE = \\sum (pred_i-y_i)^2 + \\alpha*\\sum |W_i|$$  \n",
    "\n",
    "\n",
    "Здесь $\\alpha$ -- заранее задаваемый гиперпараметр. Это вес, с которым второе слагаемое будет влиять на лосс."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-_w_qDo2zmBA"
   },
   "source": [
    "Добавление регуляризации, как правило, помогает бороться с **переобучением**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b0Ggld74zmBB"
   },
   "source": [
    "Подробнее об этом можно почитать [тут](http://www.machinelearning.ru/wiki/index.php?title=%D0%9B%D0%B0%D1%81%D1%81%D0%BE) и [тут](http://www.machinelearning.ru/wiki/index.php?title=%D0%A0%D0%B8%D0%B4%D0%B6-%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-5_oOJCzmBB"
   },
   "source": [
    "### Полезные ссылки:\n",
    "1. Открытый курс по машинному обучению: https://habr.com/company/ods/blog/323890/\n",
    "2. Если вам интересно математическое обоснование всех методов, рекомендуем ознакомиться с этой книгой: https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[homework]gradient_linear_model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
